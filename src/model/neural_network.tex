Departing from the previous research of the problem, a distinctive aspect of the model of this work is the generation of smooth advection fields as opposed to pixel-wise advection vector estimation.
The chosen approach is better suited especially for the high resolution meteorological applications in mind as small-scale differences in the data are mostly random given the available measurements, and features in the data mostly move to the same direction.

The data fed into the neural network consists of some smooth objects moving across two spatial dimensions over time.
Using several consecutive spatial snapshots of this data, the network predicts an advection field that represents the advection process across these time steps.
Even in the presence of noisy data, the objective is to produce smooth advection fields.
While the network is trained and evaluated mainly on satellite images of COT data, it is intended to be applicable to all similar processes.

Advection field estimation has also been studied in the context of optical flow problem.
Classical optical flow techniques are also often used in meteorology in nowcasting problems~\cite{pulkkinenetal}.
The neural network approach has also been studied but mostly focusing on different objectives.
They are often trained with ground truth flows and with datasets where motion only applies to small objects in images most notably with the KITTI~\cite{kitti}, and Sintel~\cite{sintel} datasets.
Contrary to this, the model used for this thesis has to be trained in an unsupervised fashion as the ground truth advection fields are not available, and the data for which the model is intended to be used with is very different from the usual ones used in the neural network optical flow context.

In order to tackle the needs of the much lower-dimensional data similar to the one described in Section~\ref{subsubsection:dynamicmodellingspatiotemporaldata} and in Figure~\ref{fig:advection_diffusion}, a different kind of approach to the neural network structure is needed.
Basis vectors are utilised here akin to the method used in~\cite{deepide}, but a simple second degree polynomial is opted as opposed to a high-dimensional radial basis for the advection field.
The choice of outputting a low-dimensional polynomial basis from the neural network guarantees smooth advection fields. 

\subsubsection{Model Architecture}
The model architecture is similar to the well-known Residual Network (ResNet) that is deep yet computationally efficient.
The model compromises of two separate ResNet models, each trained to predict a single of the two-dimensional advection field: one for the u-component, and the other for the v-component.

\subsubsection{ResNet Architecture}
Residual Networks are characterised by the use of skip connections, which allow the gradient to be directly backpropagated to earlier layers.
The ‘residual’ term comes from the learning process of these networks where they learn the residual mappings relative to the layer inputs, alleviating the common vanishing gradient problem in deep neural networks.
While residual connections are utilised in a wide array of neural network architectures, they were first formally developed for image recognition tasks in~\cite{resnet}.
The task of learning basis function coefficients in this thesis bears resemblance to these image recognition tasks.
However, instead of outputting classes from multi-channel images, in this thesis, the network is interpreted as outputting basis function coefficients of advection field from spatio-temporal data.

A key component of the ResNet architecture is the residual block.
Residual blocks have the general form
\begin{equation}\label{eq:residualblock}
    F(x) + x,
\end{equation}
where $F(x)$ is the residual mapping to be learned representing layers of the neural network. $x$ is the original input to the block which represents the skip connection.
The important aspect here is the behaviour of the residual blocks in backpropagation.
Deep neural networks suffer from a problem called diminishing gradients when the chain rule results in a large number of multiplication of small numbers and thus producing small gradients.
However, the derivative of the residual block
\begin{equation}\label{eq:residualblockderivative}
    \frac{d}{dx} F(x) + x = \frac{d}{dx} F(x) + 1,
\end{equation}
ensures that even with propagating gradients through a deep network, they don't vanish, thanks to the $+1$ term.

This kind of technique is applied in a wide array of neural network architectures, but perhaps the most common use is in convolutional neural networks.
In convolutional networks, the residual blocks consist of several convolution layers, each with batch normalisations and activations.
In this thesis, a 26-layer convolutional ResNet was deployed.
The specific layer architecture of the used network is shown in Figure~\ref{fig:resnetmodel}.

\begin{figure}[h]
    \centering
    \begin{footnotesize}
        \input{model/fig/resnet_model.tex}
    \end{footnotesize}
    \caption[LoF entry]{\label{fig:resnetmodel}The structure of the used 26-layer ResNet architecture.
    The network takes as an input spatio-temporal data with temporal dimension $\tau$, and spatial dimension $(ny,nx)$, and outputs a vector of length $n_{\text{basis}}$.

    \hspace*{1em}Here the residual blocks that sum the block input to its output are the groups of layers contained within brackets.

    \hspace*{1em}The layer parameters are interpreted as the used kernel size followed by the layer type, number of output channels, and stride.
    Striding divides the spatial dimension by the used parameter by skipping that number of pixels when computing the results.
    Maxpool layer outputs the maximum value inside the used kernel size, and average pool takes the average value.
    In the last layers, adaptive average pool adjusts the kernel size such that the output shape of the layer is $512 \times 1 \times 1$, and linear layer is just a matrix multiplication to produce an $n_{\text{basis}}$-vector.
}
\end{figure}

\subsubsection{Producing Advection Fields}
While the described architecture generated the low-dimensional basis vector coefficients, the primary objective is to produce the complete advection fields.
To this end, the basis vectors are constructed as second-degree polynomials across the spatial data domain.
The choice of second-degree polynomials are chosen to strike a balance between capturing a majority of the underlying advection process and maintaining sufficient flexibility to span over multiple time steps.
Thus the advection fields are now of the form
\begin{equation}
    \mathbf{F}(x,y) = 
    \begin{bmatrix}
        a_{u_0} + a_{u_1} x + a_{u_2} y + a_{u_3} x^2 + a_{u_4} xy + a_{u_5} y^2\\
        a_{v_0} + a_{v_1} x + a_{v_2} y + a_{v_3} x^2 + a_{v_4} xy + a_{v_5} y^2\\
    \end{bmatrix}.
\end{equation}

Employing two separate ResNet models, the coefficient vectors $a_u$, and $a_v$ can be obtained from data.
Assuming a square spatial domain $\R^{n_x \times n_y}$, where $n_x = n_y$, the Vandermonde matrix for the two-dimensional second-degree polynomials can be constructed as
\begin{equation}
    V = V((x_0,y_0),(x_1,y_1),\dots,(x_{nx}, y_{ny})) =
    \begin{bmatrix}
        1 & x_0 & y_0 & x_{0}^2 & x_{0}y_{0} & y_{0}^2\\
        1 & x_1 & y_1 & x_{1}^2 & x_{1}y_{1} & y_{1}^2\\
        \vdots & \vdots & \vdots & \vdots & \vdots & \vdots \\
        1 & x_{n_x} & y_{n_y} & x_{n_x}^2 & x_{n_x}y_{n_y} & y_{n_y}^2
    \end{bmatrix}.
\end{equation}
Given this, the advection field can be expressed as:
\begin{equation}
    \mathbf{F} = 
    \begin{bmatrix}
        Va_u \\
        Va_v
    \end{bmatrix}.
\end{equation}

Since the basis vectors remain constant, the model complexity is effectively controlled by the dimensionality of the basis.
Consequently, the selection of the basis represents a trade-off between smoothness of the advection fields and the ability to fit more complex patterns in the advection fields.

\subsubsection{Training the Neural Network}
An unsupervised training approach is employed due to the general unavailability of ground-truth advection fields.
The loss computation in training has to be thus done trough warping the latest data sample according to the predicted advection field, and comparing it to the true data samples of corresponding time steps.

The warping operation essentially transforms the data coordinates in alighment with the predicted advection fields.
During training phase, bilinear interpolation is applied to execute this operation.
This method calculates the value at a new location in the warped data as a weighted average of the values from the nearest four locations in the original data, ensuring differentiability for backpropagation.
However, this method also tends to smooth out the warped images.
Consequently, a simpler nearest-point interpolation method is used in inference.

The network is trained by minimising the average MSE loss between warped images, and the target images over a predetermined number of time steps.
For each iteration, the network predicts the advection fields from a batch of $\tau$ consecutive data samples, and the latest image from this batch is warped using the aforementioned warping scheme.
The warping is done over several subsequent time steps to compute an average loss between actual images, and the warped images.
Due to the lack of exterior information beyound the image boundaries, the model cannot predict the edge values.
Therefore, the edge values are discarded when computing the MSE loss.
The selection of the $\tau$ and the number of time steps to warp in training depend on the temporal characteristics of the training data.

As the optimisation algorithm, Adam is employed in optimising the model.
It is a variant of the already discussed SGD method but due to its adaptive step size it is often more efficient than the SGD.
